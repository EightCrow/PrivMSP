# 软提示训练过程分析

## 1. 软提示（Soft Prompt）概述

软提示是一种参数高效的微调方法，它不需要更新大型语言模型的所有参数，而是通过添加和优化一小组可学习的嵌入向量（embeddings）来适应特定任务。在MSP（Mixture of Soft Prompts）项目中，软提示被用于控制文本生成的属性，实现多属性可控的数据生成。

## 2. 软提示的基本结构

项目中实现了几种不同类型的软提示嵌入：

### 2.1 基础软嵌入类 (SoftEmbedding)

```python
class SoftEmbedding(nn.Module):
  def __init__(self, original_emb: nn.Embedding, n_tokens: int=10, num_exemplars: int=5,
              init_from_vocab: bool=True, tokenizer=None):
    super().__init__()
    self.name = 'base-embedding'
    self.original_emb = original_emb
    self.n_tokens = n_tokens

    init_text = f"Show me {num_to_string[num_exemplars + 1]} distinct utterances that all express the "
    init_prompt_value = self.init_embedding(
      original_emb, n_tokens, init_from_vocab, tokenizer, init_text
    )
    self.soft_prompt = nn.Parameter(init_prompt_value, requires_grad=True).to(device)
```

这个基础类定义了软提示的核心功能：
- 初始化一组可学习的嵌入向量
- 可以从词汇表中初始化或随机初始化
- 提供保存和加载软提示的方法

### 2.2 因果关系嵌入 (CausalEmbedding)

```python
class CausalEmbedding(SoftEmbedding):
  def forward(self, tokens):
    batch_size, seq_len = tokens.shape
    # use soft prompt unless we are using the autoregressive `.generate()`
    if seq_len > 1:
      input_embed = self.original_emb(tokens[:, self.n_tokens:])
      learned_embed = self.soft_prompt.repeat(batch_size, 1, 1)
      final_embed = torch.cat([learned_embed, input_embed], 1)
    else:
      final_embed = self.original_emb(tokens)
    return final_embed
```

这个类专为自回归（因果）语言模型设计，如GPT系列。它在前向传播时将软提示嵌入与输入嵌入连接起来。

### 2.3 序列到序列嵌入 (Seq2SeqEmbedding)

```python
class Seq2SeqEmbedding(SoftEmbedding):
  def forward(self, tokens):
    if tokens[0][0] < 0:  # if first token is a soft prompt placeholder
      input_embed = self.original_emb(tokens[:, self.n_tokens:])
      learned_embed = self.soft_prompt.repeat(tokens.shape[0], 1, 1)
      final_embed = torch.cat([learned_embed, input_embed], 1)
    else:
      final_embed = self.original_emb(tokens)
    return final_embed
```

这个类专为序列到序列模型设计，如T5或BART。它通过检查第一个token是否为占位符来决定是否使用软提示。

### 2.4 属性嵌入 (AttributeEmbedding)

```python
class AttributeEmbedding(nn.Module):
  def __init__(self, args, attributes: list, original_emb: nn.Embedding, num_sets: int=1, 
          frozen: bool=False, tokenizer=None, attribute_init_texts=None):
    # ...
    self.mixture_type = args.mixture
    self.model_type = args.model

    if self.mixture_type == 'attention':
      self.attention = AttributeAttention(original_emb.weight.size(1), args.temperature)
    elif self.mixture_type == 'bottleneck':
      self.bottleneck = AttributeBottleneck(original_emb.weight.size(1), args.hidden_size, args.temperature)
    elif self.mixture_type == 'cnn':
      self.cnn_mixture = AttributeConvolution(original_emb.weight.size(1), stack_height=stack_height)
```

这个类是MSP的核心，它实现了多属性的嵌入和混合。它支持多种混合方式：
- 连接（concat）：简单地将多个属性嵌入连接起来
- 注意力（attention）：使用注意力机制混合属性
- 瓶颈（bottleneck）：通过瓶颈层混合属性
- CNN：使用卷积神经网络混合属性
- 池化（pooling）：使用平均池化混合属性

## 3. 软提示训练流程

软提示的训练过程主要在`main.py`中的`run_prompt_train`函数中实现：

```python
def run_prompt_train(args, model, datasets, exp_logger, ontology):
  # 冻结大型语言模型的参数
  parameters = list(model.parameters())
  for param in parameters:
    param.requires_grad = False

  # 创建并设置软提示嵌入
  if args.model == 'gpt':
    soft_prompt_embed = CausalEmbedding(model.get_input_embeddings(), args.n_tokens)
  else:
    soft_prompt_embed = Seq2SeqEmbedding(model.get_input_embeddings(), args.n_tokens)
  model.set_input_embeddings(soft_prompt_embed)

  # 训练软提示
  model = run_train_loop(args, model, datasets, exp_logger, soft_prompt_embed)
  return model
```

训练过程的关键步骤包括：

1. **冻结预训练模型参数**：将大型语言模型的所有参数设为不可训练，确保只有软提示被更新。

2. **创建软提示嵌入**：根据模型类型（GPT或序列到序列）创建相应的软提示嵌入。

3. **替换模型的嵌入层**：将模型的原始嵌入层替换为包含软提示的嵌入层。

4. **训练循环**：在`run_train_loop`函数中进行实际的训练。

### 3.1 训练循环详解

训练循环在`run_train_loop`函数中实现：

```python
def run_train_loop(args, model, datasets, exp_logger, soft_embeds=None):
  dataset, dev_dataset = datasets['train'], datasets['dev']
  train_dataloader = get_dataloader(args, dataset)
  
  # 设置优化器和学习率调度器
  if soft_embeds:
    optimizer, scheduler = setup_optimization(args, soft_embeds, total_steps)
  else:
    optimizer, scheduler = setup_optimization(args, model, total_steps)

  for epoch_count in range(exp_logger.num_epochs):
    exp_logger.start_epoch(train_dataloader)
    model.train()

    for step, batch in enumerate(train_dataloader):
      inputs, targets = dataset.collate(args, batch)
      
      # 前向传播
      with autocast(dtype=torch.bfloat16):
        outputs = model(**inputs, labels=targets)
        loss = outputs.loss / args.grad_accum_steps
      
      # 反向传播
      loss = scaler.scale(loss)
      loss.backward()

      # 更新参数
      if (step + 1) % args.grad_accum_steps == 0:
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
        model.zero_grad()
      
      # 记录和评估
      exp_logger.log_train(step, scheduler)
      
    # 在验证集上评估
    eval_res = run_eval(args, model, dev_dataset, exp_logger)
    if eval_res[exp_logger.metric] >= exp_logger.best_score[exp_logger.metric]:
      exp_logger.best_score = eval_res
      if soft_embeds:
        exp_logger.save_best_soft_prompt(args, soft_embeds)
```

训练循环的关键步骤包括：

1. **数据准备**：使用`get_dataloader`函数获取训练数据加载器。

2. **优化器设置**：使用`setup_optimization`函数设置优化器和学习率调度器，只优化软提示参数。

3. **训练循环**：
   - 对每个批次的数据进行前向传播
   - 计算损失并进行反向传播
   - 更新软提示参数
   - 记录训练进度

4. **评估和保存**：
   - 在验证集上评估模型性能
   - 保存性能最佳的软提示

### 3.2 优化器设置

优化器设置在`utils/help.py`的`setup_optimization`函数中实现：

```python
def setup_optimization(args, model, total_steps):
  no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
  if args.task == 'soft_prompt':
    optimizer_grouped_parameters = model.parameters() # model是软提示嵌入
  else:
    optimizer_grouped_parameters = [
      {"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
        "weight_decay": args.weight_decay,
      },
      {"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
        "weight_decay": 0.0},
    ]

  warmup = int(total_steps * args.warmup_steps)
  optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, weight_decay=args.weight_decay)
  scheduler = get_scheduler(schedule_type, optimizer, num_warmup_steps=warmup, num_training_steps=total_steps)
  return optimizer, scheduler
```

对于软提示训练，优化器只优化软提示参数，而不是整个模型的参数。

## 4. 软提示混合（MSP核心）

MSP的核心是软提示混合，它在`utils/synthesize.py`的`soft_prompt_mixing`函数中实现：

```python
def soft_prompt_mixing(args, model, datasets, exp_logger, ontology):
  # 冻结大型语言模型
  for param in model.parameters():
    param.requires_grad = False

  # 创建指令提示和属性嵌入
  original_emb = model.get_input_embeddings()
  if args.model == 'gpt':
    instruction_prompt = CausalEmbedding(original_emb, args.n_tokens, args.num_shot, tokenizer=tokenizer)
  else:
    instruction_prompt = Seq2SeqEmbedding(original_emb, args.n_tokens, args.num_shot, tokenizer=tokenizer)

  attribute_embeddings = setup_attribute_embeddings(args, original_emb, ontology, tokenizer)
  attribute_embeddings.instruction_prompt = instruction_prompt
  model.set_input_embeddings(attribute_embeddings)
  
  # 训练循环
  for epoch_count in range(exp_logger.num_epochs):
    for step, batch in enumerate(train_dataloader):
      inputs, targets, metadata = dataset.collate(args, batch)
      attribute_embeddings.set_constraints(metadata)
      
      # 前向传播和反向传播
      outputs = model(**inputs, labels=targets)
      loss = outputs.loss / args.grad_accum_steps
      loss.backward()
      
      # 更新参数
      if (step + 1) % args.grad_accum_steps == 0:
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
        model.zero_grad()
```

软提示混合的关键步骤包括：

1. **创建指令提示**：创建一个基础的指令提示嵌入。

2. **创建属性嵌入**：为每个属性创建一个嵌入，并将它们组织在一起。

3. **设置约束**：在每个批次中，根据元数据设置约束（即要控制的属性）。

4. **训练循环**：训练属性嵌入和混合机制。

## 5. 使用训练好的软提示生成数据

训练好的软提示可以用于生成数据，这在`components/ct_generator.py`的`SoftPromptMixer`类中实现：

```python
class SoftPromptMixer(ControlledTextGenerator):
  def __init__(self, args, pieces):
    self.generator = pieces['model']
    self.attribute_embedding = pieces['attribute_embedding']
    self.tokenizer = pieces['tokenizer']
    self.engineer = pieces['prompt_engineer']
    self.mixture_type = args.mixture

  def synthesize(self, seed_example, constraints):
    # 准备输入
    prompt_text = self.engineer.prompt_with_exemplars(seed_example, add_tail=True, include_seed=True)
    tokens = self.tokenizer(prompt_text, return_tensors='pt')
    
    # 添加指令提示
    instruct_tokens = torch.full((self.prefix_len,), -1, dtype=content.dtype)
    input_ids = torch.cat([instruct_tokens, content]).unsqueeze(0).to(device)
    
    # 设置约束（要控制的属性）
    metadata = {'constraints': [constraints] * beams, 'pad_lengths': [0] * beams}
    self.attribute_embedding.set_constraints(metadata)
    
    # 生成文本
    outputs = self.generator.generate(**inputs, max_new_tokens=self.new_tokens, do_sample=True,
                            num_return_sequences=beams, repetition_penalty=self.rep_penalty, 
                            temperature=self.temp, num_beams=beams, early_stopping=True)
    
    # 处理生成的文本
    output_strings = self.tokenizer.batch_decode(outputs.detach())
    generations = []
    for output in output_strings:
      # 处理生成的文本...
      generations.append(generated_str.strip())
    
    return generations
```

生成过程的关键步骤包括：

1. **准备输入**：使用种子示例和提示工程师创建输入提示。

2. **添加软提示**：在输入前添加指令提示占位符。

3. **设置约束**：设置要控制的属性。

4. **生成文本**：使用训练好的模型和软提示生成文本。

5. **处理输出**：处理生成的文本，去除不需要的部分。

## 6. 软提示训练的优势

1. **参数高效**：只需要训练少量参数（软提示），而不是整个大型语言模型。

2. **可控性**：通过混合不同属性的软提示，可以控制生成文本的多个属性。

3. **灵活性**：支持多种混合方式（连接、注意力、瓶颈、CNN、池化），可以根据任务需求选择最合适的方式。

4. **可扩展性**：可以轻松添加新的属性，而不需要重新训练整个模型。

## 7. 总结

MSP项目中的软提示训练是一种参数高效的方法，用于控制大型语言模型的生成行为。通过训练和混合不同属性的软提示，可以生成具有特定属性的文本，这对于数据增强和少样本学习特别有用。软提示训练的核心是冻结预训练模型的参数，只更新软提示嵌入，这大大减少了计算资源的需求，同时保持了生成文本的质量。
