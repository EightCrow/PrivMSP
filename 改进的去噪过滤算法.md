# 改进的去噪过滤算法

## 算法概述

本算法是对MSP项目中原有去噪过滤机制的改进，通过结合聚类分析和多策略分布调整方法，实现了一个可参数化的过滤系统。该系统能够通过设置不同参数来影响过滤后合成数据的分布特征，支持多种分布调整策略，包括拟合原始数据分布、均衡各类数据分布、强调稀有属性等。

## 算法伪代码

```
算法：参数化去噪过滤 (Parameterized Denoising Filter)

输入:
- 初始合成数据 D_syn
- 真实数据 D_real
- 嵌入模型 φ
- 聚类数量 K
- 目标合成样本数 T
- 分布控制参数 α (0-1之间，控制均衡度，0表示完全拟合原始分布，1表示完全均衡分布)
- 稀有属性增强因子 β (≥1，控制稀有属性的采样权重)
- 语义相似度阈值 γ (0-1之间，控制语义过滤严格程度)

输出:
- 过滤后的合成数据 D_filtered

步骤:
1. 计算合成数据和真实数据的嵌入表示:
   {e_i^syn}_{i=1}^{|D_syn|} = φ(D_syn)
   {e_i^real}_{i=1}^{|D_real|} = φ(D_real)

2. 运行k-means将{e_i^syn}聚类为K组。令第i组为G_i，对应的质心为c_i。

3. 计算属性频率分布:
   freq_ratio = attribute_frequency(D_real)
   
4. 初始化K维直方图h:
   对于每个真实样本x_i ∈ D_real:
     γ = argmin_{j=1}^K |e_i^real - c_j|
     h[γ] = h[γ] + 1

5. 计算目标分布p:
   p_original = h/|D_real|  // 原始分布
   p_uniform = [1/K, 1/K, ..., 1/K]  // 均匀分布
   p = (1-α) * p_original + α * p_uniform  // 混合分布

6. 应用稀有属性增强:
   对于每个聚类G_i:
     计算G_i中样本的平均属性频率avg_freq_i
     稀有度score_i = 1/avg_freq_i
     p[i] = p[i] * (1 + β * (稀有度score_i - 1))
   重新归一化p使其和为1

7. 根据调整后的分布p重采样合成数据:
   D_filtered = []
   对于每个聚类G_i:
     target_count = max(⌊T * p[i]⌋, 0)
     如果|G_i| < target_count:
       返回"需要更多初始样本"
     否则:
       从G_i中均匀采样target_count个样本，添加到D_filtered
       
8. 应用语义相似度过滤:
   对于D_filtered中的每个样本x:
     计算x与其对应原始样本的语义相似度sim
     如果sim < γ:
       从D_filtered中移除x

9. 返回D_filtered
```

## 算法实现

以下是基于MSP项目中现有`denoise.py`的改进实现：

```python
def parameterized_filter(args, synthetic_data, seed_data):
    """
    参数化去噪过滤算法
    
    参数:
    - args: 包含以下字段的参数对象:
        - embedding_model: 嵌入模型名称
        - num_clusters: 聚类数量K
        - target_samples: 目标合成样本数T
        - distribution_alpha: 分布控制参数α
        - rarity_beta: 稀有属性增强因子β
        - similarity_threshold: 语义相似度阈值γ
        - distribution_type: 分布类型('original', 'uniform', 'balanced', 'rare_enhanced')
    - synthetic_data: 初始合成数据列表
    - seed_data: 真实数据列表
    
    返回:
    - filtered_data: 过滤后的合成数据列表
    - distribution_info: 分布信息字典，用于可视化
    """
    # 0. 展平合成数据
    flat_synthetic_data = []
    for synth_group in synthetic_data:
        flat_synthetic_data.extend(synth_group)
    
    # 1. 加载嵌入模型
    embedder_model = SentenceTransformer(args.embedding_model, device=device)
    
    # 2. 计算嵌入表示
    synthetic_texts = [example['text'] for example in flat_synthetic_data]
    real_texts = [example['text'] for example in seed_data]
    
    synthetic_embeddings = embedder_model.encode(synthetic_texts)
    real_embeddings = embedder_model.encode(real_texts)
    
    # 3. 运行k-means聚类
    kmeans = KMeans(n_clusters=args.num_clusters, random_state=42)
    kmeans.fit(synthetic_embeddings)
    
    synthetic_clusters = kmeans.labels_
    cluster_centers = kmeans.cluster_centers_
    
    # 4. 将真实数据分配到最近的聚类
    real_clusters = []
    for emb in real_embeddings:
        distances = np.linalg.norm(cluster_centers - emb, axis=1)
        nearest_cluster = np.argmin(distances)
        real_clusters.append(nearest_cluster)
    
    # 5. 初始化直方图并计算原始分布
    histogram = np.zeros(args.num_clusters)
    for cluster in real_clusters:
        histogram[cluster] += 1
    
    # 6. 计算目标分布
    original_distribution = histogram / np.sum(histogram)
    uniform_distribution = np.ones(args.num_clusters) / args.num_clusters
    
    if args.distribution_type == 'original':
        target_distribution = original_distribution
    elif args.distribution_type == 'uniform':
        target_distribution = uniform_distribution
    elif args.distribution_type == 'balanced':
        target_distribution = (1 - args.distribution_alpha) * original_distribution + args.distribution_alpha * uniform_distribution
    
    # 7. 应用稀有属性增强（如果需要）
    if args.distribution_type == 'rare_enhanced':
        cluster_rarities = np.zeros(args.num_clusters)
        cluster_counts = np.zeros(args.num_clusters)
        
        for i, example in enumerate(flat_synthetic_data):
            cluster = synthetic_clusters[i]
            rarity = calculate_sample_rarity(example, frequency_ratio)
            cluster_rarities[cluster] += rarity
            cluster_counts[cluster] += 1
        
        cluster_counts = np.maximum(cluster_counts, 1)
        avg_cluster_rarities = cluster_rarities / cluster_counts
        enhancement_factor = 1 + args.rarity_beta * (avg_cluster_rarities - 1)
        target_distribution = target_distribution * enhancement_factor
        target_distribution = target_distribution / np.sum(target_distribution)
    
    # 8. 重采样合成数据
    filtered_data = []
    for cluster_id in range(args.num_clusters):
        cluster_indices = np.where(synthetic_clusters == cluster_id)[0]
        cluster_samples = [flat_synthetic_data[i] for i in cluster_indices]
        target_count = int(args.target_samples * target_distribution[cluster_id])
        
        if len(cluster_samples) < target_count:
            print(f"警告: 聚类 {cluster_id} 没有足够的样本")
            sampled_indices = list(range(len(cluster_samples)))
        else:
            sampled_indices = random.sample(range(len(cluster_samples)), target_count)
        
        # 应用语义相似度过滤
        for idx in sampled_indices:
            sample = cluster_samples[idx]
            max_similarity = max(semantic_similarity(sample, real_sample, embedder_model)
                               for real_sample in seed_data)
            if max_similarity >= args.similarity_threshold:
                filtered_data.append(sample)
    
    return filtered_data
```

## 可视化分布

为了帮助用户理解过滤前后的数据分布变化，我们提供了可视化模块：

```python
def visualize_distributions(distribution_info, save_path=None):
    """可视化原始分布、目标分布和实际分布"""
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        import pandas as pd
        
        clusters = list(range(len(distribution_info['original_distribution'])))
        df = pd.DataFrame({
            'Cluster': np.repeat(clusters, 3),
            'Distribution': ['Original', 'Target', 'Actual'] * len(clusters),
            'Probability': (distribution_info['original_distribution'] + 
                          distribution_info['target_distribution'] + 
                          distribution_info['actual_distribution'])
        })
        
        plt.figure(figsize=(12, 6))
        sns.barplot(x='Cluster', y='Probability', hue='Distribution', data=df)
        plt.title('Cluster Distribution Comparison')
        plt.xlabel('Cluster ID')
        plt.ylabel('Probability')
        plt.legend(title='Distribution Type')
        
        if save_path:
            plt.savefig(save_path)
            plt.close()
        else:
            plt.show()
    except ImportError:
        print("警告: 缺少可视化所需的库。")

def visualize_attribute_distribution(distribution_info, top_n=10, save_path=None):
    """可视化每个聚类中的主要属性分布"""
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        num_clusters = len(distribution_info['cluster_attributes'])
        fig, axes = plt.subplots(num_clusters, 1, figsize=(12, 5 * num_clusters))
        
        if num_clusters == 1:
            axes = [axes]
        
        for i, (cluster_attrs, ax) in enumerate(zip(distribution_info['cluster_attributes'], axes)):
            top_attrs = sorted(cluster_attrs.items(), key=lambda x: x[1], reverse=True)[:top_n]
            attrs, counts = zip(*top_attrs)
            
            sns.barplot(x=list(counts), y=list(attrs), ax=ax)
            ax.set_title(f'Cluster {i} - Top {top_n} Attributes')
            ax.set_xlabel('Count')
            ax.set_ylabel('Attribute')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path)
            plt.close()
        else:
            plt.show()
    except ImportError:
        print("警告: 缺少可视化所需的库。")
```

## 使用示例

以下是如何使用改进的去噪过滤算法的示例：

```python
# 定义参数
class Args:
    def __init__(self):
        self.embedding_model = 'all-MiniLM-L6-v2'
        self.num_clusters = 10
        self.target_samples = 1000
        self.distribution_alpha = 0.5  # 平衡原始分布和均匀分布
        self.rarity_beta = 2.0  # 稀有属性增强因子
        self.similarity_threshold = 0.7  # 语义相似度阈值
        self.distribution_type = 'balanced'  # 'original', 'uniform', 'balanced', 'rare_enhanced'

# 创建参数对象
args = Args()

# 应用过滤算法
filtered_data, distribution_info = parameterized_filter(args, synthetic_data, seed_data)

# 可视化分布
visualize_distributions(distribution_info, save_path='distribution_comparison.png')
visualize_attribute_distribution(distribution_info, top_n=10, save_path='attribute_distribution.png')

# 保存过滤后的数据
import json
with open('filtered_data.json', 'w') as f:
    json.dump(filtered_data, f, indent=2)
```

## 算法优势

1. **可参数化控制**：通过设置不同的参数，可以灵活控制过滤后数据的分布特征。

2. **多种分布策略**：
   - `original`: 保持与原始数据相似的分布
   - `uniform`: 均匀分布，确保各类数据平衡
   - `balanced`: 在原始分布和均匀分布之间平衡
   - `rare_enhanced`: 增强稀有属性的表示

3. **语义质量保证**：通过语义相似度过滤，确保合成数据的质量。

4. **可视化支持**：提供直观的分布可视化，帮助理解和调整过滤效果。

5. **灵活性**：支持不同的嵌入模型和聚类数量，可根据具体需求进行调整。

## 与MSP项目的集成

该算法可以替代MSP项目中原有的`denoise.py`中的去噪过滤机制，并在`main.py`中添加相应的参数设置和调用代码。通过这种方式，可以使MSP项目具备更强大、更灵活的数据过滤能力，从而生成更高质量、分布更可控的合成数据。 